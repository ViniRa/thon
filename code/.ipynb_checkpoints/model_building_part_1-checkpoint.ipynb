{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for building the models\n",
    "_Author: Jimmy Charité_  \n",
    "_Email: jimmy.charite@gmail.com_  \n",
    "_Date: April 22, 2017_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model building part one, I experiment with the performance of models that ignore 'bag of word' type features. Several of the classes defined in this notebook will not be used. I keep them here for use in later model building exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retval=os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data=pd.read_pickle('./clean_data/clean_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpful</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>readability</th>\n",
       "      <th>neg_senti</th>\n",
       "      <th>pos_senti</th>\n",
       "      <th>neu_senti</th>\n",
       "      <th>comp_senti</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>vec0</th>\n",
       "      <th>...</th>\n",
       "      <th>vec290</th>\n",
       "      <th>vec291</th>\n",
       "      <th>vec292</th>\n",
       "      <th>vec293</th>\n",
       "      <th>vec294</th>\n",
       "      <th>vec295</th>\n",
       "      <th>vec296</th>\n",
       "      <th>vec297</th>\n",
       "      <th>vec298</th>\n",
       "      <th>vec299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>6.742881</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.853</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>product arrive label peanut actually small siz...</td>\n",
       "      <td>0.037825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015524</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.020853</td>\n",
       "      <td>-0.058746</td>\n",
       "      <td>-0.001076</td>\n",
       "      <td>-0.013715</td>\n",
       "      <td>-0.035464</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.023066</td>\n",
       "      <td>0.012566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>3.555348</td>\n",
       "      <td>6.734948</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>0.043776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010004</td>\n",
       "      <td>-0.003239</td>\n",
       "      <td>0.014308</td>\n",
       "      <td>-0.050601</td>\n",
       "      <td>-0.024100</td>\n",
       "      <td>-0.023046</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.004194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>6.743588</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.8830</td>\n",
       "      <td>get wild hair taffy order pound bag taffy enjo...</td>\n",
       "      <td>0.060040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019251</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>-0.006812</td>\n",
       "      <td>-0.055859</td>\n",
       "      <td>-0.005714</td>\n",
       "      <td>-0.024808</td>\n",
       "      <td>-0.040370</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.025422</td>\n",
       "      <td>-0.006151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>4.143135</td>\n",
       "      <td>6.742527</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.9346</td>\n",
       "      <td>saltwater taffy great flavor soft chewy candy ...</td>\n",
       "      <td>0.039160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011348</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.039322</td>\n",
       "      <td>-0.005104</td>\n",
       "      <td>-0.032976</td>\n",
       "      <td>-0.014450</td>\n",
       "      <td>0.013952</td>\n",
       "      <td>0.020636</td>\n",
       "      <td>0.006221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>6.737915</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.9487</td>\n",
       "      <td>taffy good soft chewy flavor amazing definitel...</td>\n",
       "      <td>0.044667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012392</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.046236</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-0.025405</td>\n",
       "      <td>-0.014135</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.002562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpful  num_sents  num_words  readability  neg_senti  pos_senti  \\\n",
       "0      0.0   0.693147   3.610918     6.742881      0.079      0.068   \n",
       "1      0.0   1.386294   3.555348     6.734948      0.000      0.448   \n",
       "2      0.0   1.609438   4.499810     6.743588      0.029      0.163   \n",
       "3      0.0   1.609438   4.143135     6.742527      0.034      0.273   \n",
       "4      0.0   1.609438   3.526361     6.737915      0.000      0.480   \n",
       "\n",
       "   neu_senti  comp_senti                                         text_lemma  \\\n",
       "0      0.853     -0.1027  product arrive label peanut actually small siz...   \n",
       "1      0.552      0.9468  great taffy great price wide assortment yummy ...   \n",
       "2      0.809      0.8830  get wild hair taffy order pound bag taffy enjo...   \n",
       "3      0.693      0.9346  saltwater taffy great flavor soft chewy candy ...   \n",
       "4      0.520      0.9487  taffy good soft chewy flavor amazing definitel...   \n",
       "\n",
       "       vec0    ...       vec290    vec291    vec292    vec293    vec294  \\\n",
       "0  0.037825    ...    -0.015524  0.009058  0.020853 -0.058746 -0.001076   \n",
       "1  0.043776    ...    -0.010004 -0.003239  0.014308 -0.050601 -0.024100   \n",
       "2  0.060040    ...    -0.019251 -0.003896 -0.006812 -0.055859 -0.005714   \n",
       "3  0.039160    ...    -0.011348  0.001603  0.005117 -0.039322 -0.005104   \n",
       "4  0.044667    ...    -0.012392  0.000802  0.000560 -0.046236  0.000751   \n",
       "\n",
       "     vec295    vec296    vec297    vec298    vec299  \n",
       "0 -0.013715 -0.035464  0.006317  0.023066  0.012566  \n",
       "1 -0.023046 -0.017151  0.017009  0.010729  0.004194  \n",
       "2 -0.024808 -0.040370  0.012168  0.025422 -0.006151  \n",
       "3 -0.032976 -0.014450  0.013952  0.020636  0.006221  \n",
       "4 -0.025405 -0.014135  0.013558  0.014956  0.002562  \n",
       "\n",
       "[5 rows x 309 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kept_cols=[s for s in clean_data.columns if s!='text_lemma']\n",
    "kept_cols=['helpful', 'num_sents', 'num_words', 'readability', 'neg_senti',\n",
    "       'pos_senti', 'neu_senti', 'comp_senti']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_rand_state=0\n",
    "test_size=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = (clean_data[kept_cols].iloc[:,1:]).as_matrix()\n",
    "y = (clean_data[kept_cols].iloc[:,0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, \n",
    "                                              random_state=my_rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jim/anaconda2/envs/py35/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=my_rand_state)\n",
    "smote = SMOTE(random_state=my_rand_state)\n",
    "rus = RandomUnderSampler(random_state=my_rand_state)\n",
    "tl = TomekLinks(random_state=my_rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vt = VarianceThreshold()\n",
    "threshold=[p*(1-p) for p in [0, 0.05, 0.1, 0.15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, since the formula for the variance of binary variables is p*(1-p), where p is the proportion of times that the binary variable is 1, I use the proportion to define the variance thresholds. The max variance is 0.25 at p=0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    '''\n",
    "    Custom Scaler\n",
    "    '''\n",
    "    # note: returns the feature matrix with the binary columns ordered first  \n",
    "    def __init__(self,keep_n_vars,copy=True,with_mean=True,with_std=True):\n",
    "        self.scaler_copy=copy\n",
    "        self.scaler_with_mean=with_mean\n",
    "        self.scaler_with_std=with_std\n",
    "        self.scaler = StandardScaler(copy=self.scaler_copy,with_mean=self.scaler_with_mean,\n",
    "                                     with_std=self.scaler_with_std)\n",
    "        self.keep_n_vars = keep_n_vars\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[:,:self.keep_n_vars], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        X_trans=X.copy()\n",
    "        X_trans[:,:self.keep_n_vars]=self.scaler.transform(X[:,:self.keep_n_vars],y,copy)\n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale=CustomScaler(len(X[0,:])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomTfidfTransformer(BaseEstimator,TransformerMixin): \n",
    "    '''\n",
    "    Custom TfidfVectorizer\n",
    "    '''\n",
    "    # note: returns the feature matrix with the binary columns ordered first  \n",
    "    def __init__(self,keep_n_vars):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.keep_n_vars = keep_n_vars\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.tfidf.fit(X[:,self.keep_n_vars], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        #return sp.sparse.hstack((X[:,:self.keep_n_vars],self.tfidf.transform(X[:,self.keep_n_vars])))\n",
    "        return np.concatenate((X[:,:self.keep_n_vars],\n",
    "                               self.tfidf.transform(X[:,self.keep_n_vars])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf=CustomTfidfTransformer(len(X[0,:])-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although tuning is not necessary for Naive Bayes, I pass the default parameters of those models to GridSearchCV anyway so that I can do a direct pair-wise comparison with the other models across the different steps of cross-validation.  \n",
    "\n",
    "In the interest of time, I didn't use the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_clf=GaussianNB()\n",
    "priors=[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qda_clf=QuadraticDiscriminantAnalysis()\n",
    "reg_param=[0.0, 0.25, 0.5, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_clf=LogisticRegression(penalty='l2')\n",
    "C=[0.001 , 0.01, 10, 100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_clf=KNeighborsClassifier(n_jobs=4)\n",
    "n_neighbors=[2,3,4,5,6,7]\n",
    "weights=['uniform','distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf=RandomForestClassifier()\n",
    "n_estimators=[100,200]\n",
    "max_features=[.1,.3,.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree=DecisionTreeClassifier(max_depth=None, min_samples_split=2)\n",
    "bagTree_clf=BaggingClassifier(base_estimator=dtree)\n",
    "max_samples=[.3,.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight=['balanced']\n",
    "class_weight.extend([{1: w} for w in [1, 2, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn import pipeline #needed if mixing imblearn with sklearn classes\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_jobs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "skfold = StratifiedKFold(n_splits=n_folds,random_state=my_rand_state, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_clf_b = pipeline.Pipeline(steps=[('vt',vt),('scale',std_scale),('clf',nb_clf)])\n",
    "nb_clf_est_b = GridSearchCV(estimator=nb_clf_b,cv=skfold,\n",
    "              scoring='roc_auc',n_jobs=n_jobs,\n",
    "              param_grid=dict(vt__threshold=threshold,clf__priors=priors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qda_clf_b = pipeline.Pipeline(steps=[('vt',vt),('scale',std_scale),('clf',qda_clf)])\n",
    "qda_clf_est_b = GridSearchCV(estimator=qda_clf_b,cv=skfold,\n",
    "              scoring='roc_auc',n_jobs=n_jobs,\n",
    "              param_grid=dict(vt__threshold=threshold,clf__reg_param=reg_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_clf_b = pipeline.Pipeline(steps=[('vt',vt),('scale',std_scale),('clf',log_clf)])\n",
    "log_clf_est_b = GridSearchCV(estimator=log_clf_b,cv=skfold,\n",
    "              scoring='roc_auc',n_jobs=n_jobs,\n",
    "              param_grid=dict(vt__threshold=threshold,clf__C=C,\n",
    "              clf__class_weight=class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_b = pipeline.Pipeline(steps=[('vt',vt),('scale',std_scale),('clf',knn_clf)])\n",
    "knn_clf_est_b = GridSearchCV(estimator=knn_clf_b,cv=skfold,\n",
    "              scoring='roc_auc',n_jobs=n_jobs,\n",
    "              param_grid=dict(vt__threshold=threshold,\n",
    "              clf__n_neighbors=n_neighbors,\n",
    "              clf__weights=weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_clf_b = pipeline.Pipeline(steps=[('vt',vt),('scale',std_scale),('clf',rf_clf)])\n",
    "rf_clf_est_b = GridSearchCV(estimator=rf_clf_b,cv=skfold,\n",
    "              scoring='roc_auc',n_jobs=n_jobs,\n",
    "              param_grid=dict(vt__threshold=threshold,\n",
    "              clf__n_estimators=n_estimators,\n",
    "              clf__max_features=max_features,\n",
    "              clf__class_weight=class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagged Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagTree_clf_b = pipeline.Pipeline(steps=[('vt',vt),('scale',std_scale),('clf',bagTree_clf)])\n",
    "bagTree_clf_est_b = GridSearchCV(estimator=bagTree_clf_b,cv=skfold,\n",
    "              scoring='roc_auc',n_jobs=n_jobs,\n",
    "              param_grid=dict(clf__n_estimators=n_estimators,\n",
    "              clf__max_samples=max_samples,\n",
    "              clf__max_features=max_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Estimators: no bag of words or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./other_output/nb_clf_est_b.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf_est_b.fit(X_train,y_train)\n",
    "joblib.dump(nb_clf_est_b, './other_output/nb_clf_est_b.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./other_output/qda_clf_est_b.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda_clf_est_b.fit(X_train,y_train)\n",
    "joblib.dump(qda_clf_est_b, './other_output/qda_clf_est_b.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./other_output/log_clf_est_b.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf_est_b.fit(X_train,y_train)\n",
    "joblib.dump(log_clf_est_b, './other_output/log_clf_est_b.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./other_output/knn_clf_est_b.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf_est_b.fit(X_train,y_train)\n",
    "joblib.dump(knn_clf_est_b, './other_output/knn_clf_est_b.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./other_output/rf_clf_est_b.pkl']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_est_b.fit(X_train,y_train)\n",
    "joblib.dump(rf_clf_est_b, './other_output/rf_clf_est_b.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagTree_clf_est_b.fit(X_train,y_train)\n",
    "joblib.dump(bagTree_clf_est_b, './other_output/bagTree_clf_est_b.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Estimators: no bag of words or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_fpr, nb_tpr, _ = roc_curve(y_test, \n",
    "                    nb_clf_est_b.predict_proba(X_test)[:,1])\n",
    "nb_roc_auc = auc(nb_fpr, nb_tpr)\n",
    "\n",
    "qda_fpr, qda_tpr, _ = roc_curve(y_test, \n",
    "                    qda_clf_est_b.predict_proba(X_test)[:,1])\n",
    "qda_roc_auc = auc(qda_fpr, qda_tpr)\n",
    "\n",
    "log_fpr, log_tpr, _ = roc_curve(y_test, \n",
    "                    log_clf_est_b.predict_proba(X_test)[:,1])\n",
    "log_roc_auc = auc(log_fpr, log_tpr)\n",
    "\n",
    "knn_fpr, knn_tpr, _ = roc_curve(y_test, \n",
    "                    knn_clf_est_b.predict_proba(X_test)[:,1])\n",
    "knn_roc_auc = auc(knn_fpr, knn_tpr)\n",
    "\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, \n",
    "                    rf_clf_est_b.predict_proba(X_test)[:,1])\n",
    "rf_roc_auc = auc(rf_fpr, rf_tpr)\n",
    "\n",
    "bagTree_fpr, bagTree_tpr, _ = roc_curve(y_test, \n",
    "                    bagTree_clf_est_b.predict_proba(X_test)[:,1])\n",
    "bagTree_roc_auc = auc(bagTree_fpr, bagTree_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nb_fpr, nb_tpr, color='cyan', linestyle='--',\n",
    "         label='NB (area = %0.2f)' % nb_roc_auc, lw=2)\n",
    "\n",
    "plt.plot(qda_fpr, qda_tpr, color='indigo', linestyle='--',\n",
    "         label='QDA (area = %0.2f)' % qda_roc_auc, lw=2)\n",
    "\n",
    "plt.plot(log_fpr, log_tpr, color='seagreen', linestyle='--',\n",
    "         label='LOG (area = %0.2f)' % log_roc_auc, lw=2)\n",
    "\n",
    "plt.plot(knn_fpr, knn_tpr, color='yellow', linestyle='--',\n",
    "         label='KNN (area = %0.2f)' % knn_roc_auc, lw=2)\n",
    "\n",
    "plt.plot(rf_fpr, rf_tpr, color='blue', linestyle='--',\n",
    "         label='RF (area = %0.2f)' % rf_roc_auc, lw=2)\n",
    "\n",
    "plt.plot(bagTree_fpr, bagTree_tpr, color='blue', linestyle='--',\n",
    "         label='Bagged Tree (area = %0.2f)' % bagTree_roc_auc, lw=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "         label='Luck')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves of Basic Models')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('./plots/ROC_Basic.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
